{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli: Ini Adalah Contoh Teks yang Akan Dikonversi Menjadi Lowercase.\n",
      "Teks setelah diubah menjadi lowercase: ini adalah contoh teks yang akan dikonversi menjadi lowercase.\n"
     ]
    }
   ],
   "source": [
    "# Contoh teks\n",
    "teks_asli = \"Ini Adalah Contoh Teks yang Akan Dikonversi Menjadi Lowercase.\"\n",
    " \n",
    "# Mengubah teks menjadi lowercase\n",
    "teks_lowercase = teks_asli.lower()\n",
    " \n",
    "# Menampilkan hasil\n",
    "print(\"Teks asli:\", teks_asli)\n",
    "print(\"Teks setelah diubah menjadi lowercase:\", teks_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli:\n",
      "\n",
      "Ini adalah contoh teks dengan campuran huruf besar dan kecil.\n",
      "Contoh ini digunakan untuk demonstrasi case folding dalam pra-pemrosesan teks.\n",
      "Dengan menggunakan case folding, semua huruf dalam teks akan diubah menjadi huruf kecil.\n",
      "Ini membantu dalam memastikan konsistensi dalam analisis teks.\n",
      "\n",
      "Teks setelah case folding:\n",
      "\n",
      "ini adalah contoh teks dengan campuran huruf besar dan kecil.\n",
      "contoh ini digunakan untuk demonstrasi case folding dalam pra-pemrosesan teks.\n",
      "dengan menggunakan case folding, semua huruf dalam teks akan diubah menjadi huruf kecil.\n",
      "ini membantu dalam memastikan konsistensi dalam analisis teks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Contoh teks dengan campuran huruf besar dan kecil\n",
    "teks_asli = \"\"\"\n",
    "Ini adalah contoh teks dengan campuran huruf besar dan kecil.\n",
    "Contoh ini digunakan untuk demonstrasi case folding dalam pra-pemrosesan teks.\n",
    "Dengan menggunakan case folding, semua huruf dalam teks akan diubah menjadi huruf kecil.\n",
    "Ini membantu dalam memastikan konsistensi dalam analisis teks.\n",
    "\"\"\"\n",
    " \n",
    "# Mengubah teks menjadi lowercase menggunakan case folding\n",
    "teks_case_folded = teks_asli.lower()\n",
    " \n",
    "# Menampilkan hasil\n",
    "print(\"Teks asli:\")\n",
    "print(teks_asli)\n",
    "print(\"Teks setelah case folding:\")\n",
    "print(teks_case_folded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks dengan angka: Ini adalah contoh teks dengan angka 12345 yang akan dihapus.\n",
      "Teks tanpa angka: Ini adalah contoh teks dengan angka  yang akan dihapus.\n"
     ]
    }
   ],
   "source": [
    "# Removal Special Characters\n",
    "# Fungsi untuk menghapus angka dari teks\n",
    "def hapus_angka(teks):\n",
    "    teks_tanpa_angka = ''.join([char for char in teks if not char.isdigit()])\n",
    "    return teks_tanpa_angka\n",
    " \n",
    "# Contoh teks dengan angka\n",
    "teks_dengan_angka = \"Ini adalah contoh teks dengan angka 12345 yang akan dihapus.\"\n",
    " \n",
    "# Memanggil fungsi untuk menghapus angka\n",
    "teks_tanpa_angka = hapus_angka(teks_dengan_angka)\n",
    " \n",
    "# Menampilkan hasil\n",
    "print(\"Teks dengan angka:\", teks_dengan_angka)\n",
    "print(\"Teks tanpa angka:\", teks_tanpa_angka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli:\n",
      "\n",
      "Dalam dunia ini, banyak hal terjadi, dari yang kecil hingga yang besar. Kita bisa melihat keindahan, tapi juga kekejaman. Ada harapan, namun juga keputusasaan. Bagaimanapun, hidup terus berjalan, tak peduli apa pun yang terjadi!\n",
      "\n",
      "\n",
      "Teks setelah menghapus tanda baca:\n",
      "\n",
      "Dalam dunia ini banyak hal terjadi dari yang kecil hingga yang besar Kita bisa melihat keindahan tapi juga kekejaman Ada harapan namun juga keputusasaan Bagaimanapun hidup terus berjalan tak peduli apa pun yang terjadi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Menhapus tanda baca dari teks\n",
    "import string\n",
    " \n",
    "def remove_punctuation(text):\n",
    "    # Membuat set yang berisi semua tanda baca\n",
    "    punctuation_set = set(string.punctuation)\n",
    " \n",
    "    # Menghapus tanda baca dari teks\n",
    "    text_without_punctuation = ''.join(char for char in text if char not in punctuation_set)\n",
    " \n",
    "    return text_without_punctuation\n",
    " \n",
    "# Contoh teks dengan banyak tanda baca\n",
    "teks_asli = \"\"\"\n",
    "Dalam dunia ini, banyak hal terjadi, dari yang kecil hingga yang besar. Kita bisa melihat keindahan, tapi juga kekejaman. Ada harapan, namun juga keputusasaan. Bagaimanapun, hidup terus berjalan, tak peduli apa pun yang terjadi!\n",
    "\"\"\"\n",
    " \n",
    "# Menghapus tanda baca dari teks\n",
    "teks_tanpa_tanda_baca = remove_punctuation(teks_asli)\n",
    " \n",
    "# Menampilkan hasil\n",
    "print(\"Teks asli:\")\n",
    "print(teks_asli)\n",
    "print(\"\\nTeks setelah menghapus tanda baca:\")\n",
    "print(teks_tanpa_tanda_baca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ini adalah contoh kalimat dengan spasi di awal dan akhir.\n"
     ]
    }
   ],
   "source": [
    "teks = \"   Ini adalah contoh kalimat dengan spasi di awal dan akhir.    \"\n",
    "teks_setelah_strip = teks.strip()\n",
    "print(teks_setelah_strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniadalahcontohkalimatdenganspasididalamnya.\n"
     ]
    }
   ],
   "source": [
    "teks_dengan_whitespace = \"Ini adalah    contoh kalimat    dengan spasi    di dalamnya.\"\n",
    "teks_tanpa_whitespace = teks_dengan_whitespace.replace(\" \", \"\")\n",
    "print(teks_tanpa_whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli: Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.\n",
      "Teks setelah filtering stopwords NLTK: Perekonomian Indonesia pertumbuhan membanggakan .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Set ulang nltk.data.path agar hanya mengandung direktori yang tepat\n",
    "nltk.data.path = [r'C:\\Users\\asus\\AppData\\Roaming\\nltk_data']\n",
    "\n",
    "# Download paket yang diperlukan\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "teks = \"Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.\"\n",
    "\n",
    "# Tokenisasi teks menjadi kata-kata\n",
    "tokens_kata = word_tokenize(teks)\n",
    "\n",
    "# Ambil daftar stopwords bahasa Indonesia\n",
    "stopwords_indonesia = set(stopwords.words('indonesian'))\n",
    "\n",
    "# Filtering kata-kata dengan menghapus stopwords\n",
    "kata_penting = [kata for kata in tokens_kata if kata.lower() not in stopwords_indonesia]\n",
    "\n",
    "# Gabungkan kata-kata penting kembali menjadi teks\n",
    "teks_tanpa_stopwords = ' '.join(kata_penting)\n",
    "\n",
    "print(\"Teks asli:\", teks)\n",
    "print(\"Teks setelah filtering stopwords NLTK:\", teks_tanpa_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ini', 'adalah', 'contoh', 'kalimat', 'untuk', 'tokenisasi', 'kata']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization\n",
    "# Misalkan kita ingin memisahkan frasa berdasarkan tanda baca koma (,)\n",
    "text = \"Ini adalah contoh kalimat untuk tokenisasi kata\"\n",
    "phrases = text.split(' ')\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ini adalah contoh kalimat pertama.', 'Dan ini adalah contoh kalimat kedua.']\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "# Contoh aturan tokenisasi khusus untuk tokenisasi kata dalam bahasa Indonesia\n",
    "import re\n",
    " \n",
    "text = \"Ini adalah contoh kalimat pertama. Dan ini adalah contoh kalimat kedua.\"\n",
    "sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apel', ' jeruk', ' pisang', ' dan mangga.']\n"
     ]
    }
   ],
   "source": [
    "# phrase tokenization\n",
    "# Misalkan kita ingin memisahkan frasa berdasarkan tanda baca koma (,)\n",
    "text = \"Apel, jeruk, pisang, dan mangga.\"\n",
    "phrases = text.split(',')\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apel', 'jeruk', 'pisang', 'dan mangga.']\n"
     ]
    }
   ],
   "source": [
    "#  hilangkan ,\n",
    "phrases = [phrase.strip() for phrase in phrases]\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pertama', 'kita', 'perlu', 'menyiapkan', 'bahan', 'bahan', 'yang', 'diperlukan']\n"
     ]
    }
   ],
   "source": [
    "# rule based tokenization\n",
    "import re\n",
    "\n",
    "text = \"Pertama, kita perlu menyiapkan bahan-bahan yang diperlukan.\"\n",
    "tokens = re.findall(r'\\w+|\\d+', text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ini', 'adalah', 'contoh', 'tokenisasi', 'berbasis', 'model.']\n"
     ]
    }
   ],
   "source": [
    "# Misalnya menggunakan spasi sebagai pemisah kata\n",
    "text = \"Ini adalah contoh tokenisasi berbasis model.\"\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'easili', 'bought', 'cri', 'leav']\n"
     ]
    }
   ],
   "source": [
    "# stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"easily\", \"bought\", \"crying\", \"leaves\"]\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'easily', 'buy', 'cry', 'leave']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"running\", \"easily\", \"bought\", \"crying\", \"leaves\"]\n",
    " \n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in words]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
